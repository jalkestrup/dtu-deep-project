{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model and Tokenizer...\n",
      "Running forward pass...\n",
      "Estimated memory usage for batch_size=256, seq_length=512: 40576 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40576"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from llm2vec.models import LlamaBiForMNTP\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def initialize_peft(\n",
    "    model,\n",
    "    lora_r: int = 16,   # Using 16 as in your config\n",
    "    lora_alpha: int = 32,  # Typically 2 * lora_r\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_modules=None\n",
    "):\n",
    "    if lora_modules is None and model.config.__class__.__name__ in [\"LlamaConfig\"]:\n",
    "        lora_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    \n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=None,  # This is customized by the actual model call.\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    return model\n",
    "\n",
    "def measure_gpu_memory(config_args):\n",
    "    # Load tokenizer and config\n",
    "    print(\"Loading Model and Tokenizer...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config_args[\"model_name_or_path\"])\n",
    "    config = AutoConfig.from_pretrained(config_args[\"model_name_or_path\"])\n",
    "    \n",
    "    # Adjust Mask Token:\n",
    "    if tokenizer.mask_token is None:\n",
    "        if config_args[\"mask_token_type\"] == \"blank\":\n",
    "            tokenizer.mask_token = \"_\"\n",
    "        elif config_args[\"mask_token_type\"] == \"eos\":\n",
    "            tokenizer.mask_token = tokenizer.eos_token\n",
    "        elif config_args[\"mask_token_type\"] == \"mask\":\n",
    "            tokenizer.add_tokens([\"<mask>\"])\n",
    "            tokenizer.mask_token = \"<mask>\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # We can use EOS as PAD token\n",
    "    \n",
    "    # Load the Bidirectional Model using LLM2Vec package\n",
    "    model_class = LlamaBiForMNTP\n",
    "    torch_dtype = torch.bfloat16 if config_args[\"torch_dtype\"] == \"bfloat16\" else torch.float16\n",
    "    model = model_class.from_pretrained(\n",
    "        config_args[\"model_name_or_path\"],\n",
    "        config=config,\n",
    "        torch_dtype=torch_dtype,\n",
    "        attn_implementation=config_args[\"attn_implementation\"]\n",
    "    )\n",
    "\n",
    "    # *** Move the model to GPU before applying anything ***\n",
    "    model.to('cuda')  # Flash Attention requires the entire model to be on GPU.\n",
    "\n",
    "    # Apply PEFT (LoRA) after moving the model to GPU\n",
    "    model.model = initialize_peft(model.model, lora_r=config_args[\"lora_r\"], lora_alpha=2*config_args[\"lora_r\"])\n",
    "\n",
    "    # Create dummy input data - respecting the batch size and sequence length from the config\n",
    "    batch_size = config_args[\"per_device_train_batch_size\"]\n",
    "    max_seq_length = config_args[\"max_seq_length\"]\n",
    "\n",
    "    dummy_input = [\"This is a test sentence for benchmarking.\" for _ in range(batch_size)]  \n",
    "    inputs = tokenizer(dummy_input, return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=\"max_length\").to('cuda')\n",
    "\n",
    "    # Clear GPU cache and measure initial memory usage\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    initial_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "    print(\"Running forward pass...\")\n",
    "    # Run a dummy forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Measure memory after the forward pass\n",
    "    final_memory = torch.cuda.memory_allocated()\n",
    "    memory_used = final_memory - initial_memory\n",
    "\n",
    "    print(f\"Estimated memory usage for batch_size={batch_size}, seq_length={max_seq_length}: {memory_used // 2**20} MB\")\n",
    "    return memory_used // 2**20\n",
    "\n",
    "# Example Config Input (typically matching what would come from JSON config)\n",
    "config_args = {\n",
    "    \"model_name_or_path\": \"princeton-nlp/Sheared-LLaMA-1.3B\",\n",
    "    \"lora_r\": 16,\n",
    "    \"per_device_train_batch_size\": 256,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"mask_token_type\": \"blank\",\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"attn_implementation\": \"flash_attention_2\"\n",
    "}\n",
    "\n",
    "# Run the check\n",
    "measure_gpu_memory(config_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above bove calculation estimates are not correct, But, running the script and calling Nvidia-smi in the console Does reveal the actual GPU footprint ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
